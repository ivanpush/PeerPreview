# Development Journal - January 20, 2025

## What We Built Today

### 1. LLM-Based PDF Parsing (Primary Work)
**Goal:** Fix broken line breaks and poor text extraction from scientific PDFs.

**What we tried:**
- Started with regex-based cleanup (failed - too many edge cases)
- Implemented GPT-4o-mini semantic cleanup per page
  - Parallel processing (3 workers)
  - Cost: ~$0.01-0.02 per 10-page paper
  - Each page sent to LLM with instructions to:
    - Remove headers/footers
    - Fix paragraph spacing
    - Join mid-sentence line breaks
    - Format section headers as `### **SECTION**`

**Files modified:**
- `backend/services/parser/pdf_parser.py` - Complete rewrite with LLM integration
- `backend/requirements.txt` - Added `openai>=1.0.0`
- `backend/.env` - Added OpenAI API key
- `backend/core/config.py` - Already had OpenAI settings

**Status:** âœ… Working but needs improvement

### 2. Issues Discovered

#### Performance Problems
- **Processing time too slow** - 10-20 seconds per document
  - 10 API calls (one per page) in parallel
  - Need to consider batching or caching common patterns

#### Quality Issues
- **Headers/footers still appearing sometimes** - LLM not catching all cases
- **Too many sections detected** - Over-splitting content
  - Example: "Figure 1" being treated as section header
  - Need better section detection logic
- **Inconsistent cleaning** - Some pages cleaned better than others

### 3. Technical Debt Created
- No progress indicators for slow uploads
- No caching of LLM responses (re-processes same paper every time)
- Frontend still has its own markdown cleaning (duplicated logic)
- Error handling could be better (just falls back to heuristics silently)

---

## Where We Need to Go Next

### Immediate Priorities (Phase 0 - PDF Parsing)

#### P0: Performance
- [ ] Add progress indicator to frontend during upload
- [ ] Implement response caching (hash-based)
  - Cache LLM responses per page hash
  - Reduces cost for re-uploads
- [ ] Consider switching to batch API (50% cheaper, 24hr turnaround)
  - Keep real-time for demo, batch for production?

#### P1: Quality Improvements
- [ ] Better section detection
  - Don't treat "Figure X" as section header
  - Require minimum content length between sections
  - Whitelist known section names only (Abstract, Introduction, Methods, Results, Discussion, References)

- [ ] Improve header/footer removal
  - Two-pass approach:
    1. LLM identifies repeated patterns across pages
    2. LLM cleans with that context
  - Or: Pre-process to find common headers before LLM calls

- [ ] Better prompt engineering
  - Give LLM examples of good vs bad cleaning
  - Tell it to preserve citations, figure numbers, table formatting
  - Explicitly tell it NOT to create new section headers

#### P2: User Experience
- [ ] Show processing status ("Cleaning page 3/10...")
- [ ] Allow cancellation of slow uploads
- [ ] Preview first page while processing rest
- [ ] Cost tracker (show user how much OpenAI credits used)

---

## Next Phase: Review Agents (Phase 1)

**Don't start until PDF parsing is solid!**

Once parsing quality is good:
1. Methods Reviewer agent
2. Figure-caption consistency agent
3. Citation Police agent

See `docs/PHASES.md` and `docs/AGENTS.md` for full plan.

---

## Technical Notes

### Current Architecture
```
PDF Upload â†’ pymupdf4llm extraction â†’ LLM cleanup (parallel) â†’ Section splitting â†’ Sentence indexing â†’ Storage
```

### Cost Analysis (per 10-page paper)
- Current: $0.01-0.02 (GPT-4o-mini, real-time)
- Batch API: $0.005-0.01 (50% savings, 24hr delay)
- Claude 3 Haiku: $0.02-0.03 (comparable)

### Files to Review
- `backend/services/parser/pdf_parser.py` - Main parsing logic
- `frontend/components/document-viewer.tsx` - Display logic (has redundant cleaning)
- `docs/prompt.md` - Original LLM parsing idea (implemented today)

---

## Lessons Learned

1. **LLMs are powerful but slow** - Trade-off between quality and speed
2. **Prompt engineering matters** - Small changes to prompt drastically affect output
3. **PDFs are messy** - Even with LLM, edge cases remain (tables, equations, figures)
4. **Cost adds up** - 10 API calls per paper, need caching ASAP
5. **User expectations** - "It's ok" means "needs work" ðŸ˜…

---

## Tomorrow's Focus

**Priority 1:** Speed up parsing
- Add progress indicator
- Implement caching

**Priority 2:** Fix section over-detection
- Stricter section header matching
- Test on multiple papers

**Priority 3:** Better prompts
- Add examples to LLM prompt
- Test different prompt variations

**Don't do:** Start building review agents yet - parsing must be rock solid first!
